data:
  train_raw: "data/toon_uwb/har_train_windows.jsonl"
  train_desc: "data/toon_uwb/descriptions_train_toon.jsonl"
  val_raw: "data/toon_uwb/har_test_windows.jsonl"
  val_desc: "data/toon_uwb/descriptions_test_toon.jsonl"
  
  train_features: "data/toon_uwb/features_train_zsym.npz"
  val_features:   "data/toon_uwb/features_test_zsym.npz"  


  use_rag_description: False
  
  fs: 1.0                # 1 Hz en tu preprocesado
  duration: 45    # ventana de 6 s

  max_desc_per_channel: 2     # cuántas frases L/B/M por canal quieres
  text_base: >
        Here I describe the sensors from activity recognition.
 

# El resto del config (modelo, optimización, etc.) lo copias de mhealth_fuzzy.yml
# y sólo cambias num_labels si quieres, o lo infieres desde train_ds.lab2id.


model:
  llama_id: TinyLlama/TinyLlama-1.1B-Chat-v1.0
  chronos_id: amazon/chronos-t5-tiny
  
  #llama_id: meta-llama/Meta-Llama-3-8B-Instruct
  #chronos_id: amazon/chronos-t5-base
  
  text_max_len: 1024
  proj_hidden: 2048
  proj_dropout: 0.2
  head_hidden: 1024
  head_dropout: 0.2
  prompt_prefix: ""
  

inference:
  llama_precision: 4bit
  chronos_precision: 4bit
  device_map_text: "cuda:0"   # si usas 8bit o multi-GPU, deja que load_models lo resuelva




train:
  epochs: 40
  batch_size: 4
  lr: 1e-5
  weight_decay: 5e-4
  label_smoothing: 0.0
  out_dir: outputs_clean
    